{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import numpy as np\r\n",
    "import tensorflow as tf\r\n",
    "from rl.agents.dqn import DQNAgent\r\n",
    "from rl.policy import EpsGreedyQPolicy\r\n",
    "from rl.memory import SequentialMemory\r\n",
    "import gym"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "n_capital = 1000\r\n",
    "\r\n",
    "class Planner(gym.Env):\r\n",
    "    def __init__(self):\r\n",
    "        self.k = np.linspace(0.01, 1.0, n_capital)\r\n",
    "        self.action_space = gym.spaces.Discrete(n_capital)\r\n",
    "        self.observation_space = gym.spaces.Discrete(n_capital)\r\n",
    "        self.decision_count = 0\r\n",
    "        self.decision_max = 100\r\n",
    "        self.observation = 500\r\n",
    "        self.alpha = 0.33\r\n",
    "\r\n",
    "    def step(self, action):\r\n",
    "        assert self.action_space.contains(action)\r\n",
    "        self.decision_count += 1\r\n",
    "        done = False\r\n",
    "        if (self.observation**self.alpha - action) > 0:\r\n",
    "            reward = np.log(self.k[self.observation]**self.alpha - self.k[action])\r\n",
    "        else:\r\n",
    "            reward = -1000\r\n",
    "        self.observation = action\r\n",
    "        if (self.decision_count >= self.decision_max) or reward == -1000:\r\n",
    "            done = True\r\n",
    "        return self.observation, reward, done, {\"decisions\": self.decision_count}\r\n",
    "\r\n",
    "    def reset(self):\r\n",
    "        self.decision_count = 0\r\n",
    "        self.observation = 500\r\n",
    "        return self.observation"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "env = Planner()\r\n",
    "\r\n",
    "model = tf.keras.models.Sequential([\r\n",
    "    tf.keras.layers.Flatten(input_shape=(1,) + env.observation_space.shape),\r\n",
    "    tf.keras.layers.Dense(32, activation=\"relu\"),\r\n",
    "    tf.keras.layers.Dense(n_capital, activation=\"linear\")\r\n",
    "])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "memory = SequentialMemory(limit=10000, window_length=1)\r\n",
    "\r\n",
    "policy = EpsGreedyQPolicy(0.30)\r\n",
    "\r\n",
    "dqn = DQNAgent(model=model, nb_actions=n_capital, memory=memory,\r\n",
    "               nb_steps_warmup=100, gamma=0.95, target_model_update=1e-2, policy=policy)\r\n",
    "\r\n",
    "dqn.compile(optimizer=tf.keras.optimizers.Adam(0.005), metrics=[\"mse\"])\r\n",
    "history = dqn.fit(env, nb_steps=10000)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training for 10000 steps ...\n",
      "Interval 1 (0 steps performed)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\Calvin\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:2426: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "10000/10000 [==============================] - 106s 11ms/step - reward: -673.3785\n",
      "done, took 105.647 seconds\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.6",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.6 64-bit"
  },
  "interpreter": {
   "hash": "ba65c22c89096d306d81d10aa6315e4a6a7c200a5c173536a89973d439e71adc"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}